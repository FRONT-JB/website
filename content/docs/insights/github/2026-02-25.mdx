---
title: "2026-02-25 GitHub 트렌딩 TOP 5"
description: "Claude Code Router 28K 스타, Wan2.1 비디오 생성 15K 스타 등 2026년 2월 25일 가장 주목받은 오픈소스 프로젝트 완전 분석"
date: 2026-02-25
---

import { GithubInfo } from "fumadocs-ui/components/github-info";
import { Step, Steps } from "fumadocs-ui/components/steps";
import { Accordion, Accordions } from "fumadocs-ui/components/accordion";

<Accordions type="single" defaultValue="ranking-info" disabled>
  <Accordion
    id="ranking-info"
    title="각 프로젝트의 누적 스타 수와 해당 날짜의 커뮤니티 관심도를 종합해 TOP 5를 선정했어요."
  />
</Accordions>

<Steps>

<Step>

### claude-code-router

<GithubInfo
  owner="musistudio"
  repo="claude-code-router"
  token={process.env.NEXT_PUBLIC_GITHUB_TOKEN}
  className="not-prose"
/>

Claude Code를 다양한 AI 프로바이더와 연결하는 프록시 라우터예요. localhost:3456에서 실행되며 OpenRouter, DeepSeek, Ollama, Gemini 등 8개 이상의 모델을 자유롭게 선택할 수 있어요. 컨텍스트 기반 라우팅으로 일반 작업, 백그라운드 작업, 추론 작업, 긴 컨텍스트 처리에 각각 최적화된 모델을 자동 할당해요. Transformer 시스템이 각 프로바이더의 API 형식 차이를 자동 변환하며, GitHub Actions와 통합해 CI/CD 파이프라인에서 저렴한 모델로 자동 코드 리뷰를 구축할 수 있어요.

<Callout type="info">
  **비용 최적화**: background 작업에 로컬 Ollama 모델을 사용하면 API 비용을 크게
  절감할 수 있어요.
</Callout>

</Step>

<Step>

### Wan2.1

<GithubInfo
  owner="Wan-Video"
  repo="Wan2.1"
  token={process.env.NEXT_PUBLIC_GITHUB_TOKEN}
  className="not-prose"
/>

알리바바가 공개한 오픈소스 비디오 생성 모델로 VBench 리더보드에서 OpenAI Sora를 능가하는 성능을 보였어요. T2V-1.3B 모델은 단 8.19GB VRAM만 필요해서 RTX 4070에서도 실행 가능하고, 5초 480p 비디오를 약 4분에 생성해요. Diffusion Transformer 아키텍처 기반이며 텍스트-비디오(T2V-14B), 이미지-비디오(I2V-14B), 경량(T2V-1.3B) 세 가지 모델을 제공해요. 중국어와 영어를 비디오 내에 자연스럽게 렌더링하고, VBench에서 Overall Score 82.7을 기록하며 Sora(81.2)를 앞질렀어요.

<Callout>
  **메모리 최적화**: Block Swap을 사용하면 14B 모델도 12GB VRAM GPU에서 실행
  가능해요.
</Callout>

</Step>

<Step>

### Spark-TTS

<GithubInfo
  owner="SparkAudio"
  repo="Spark-TTS"
  token={process.env.NEXT_PUBLIC_GITHUB_TOKEN}
  className="not-prose"
/>

홍콩과기대와 Mobvoi가 개발한 LLM 기반 TTS 시스템이에요. Qwen2.5 LLM을 직접 활용해 별도의 음향 모델 없이 토큰에서 바로 오디오를 복원해요. BiCodec 아키텍처로 음성을 언어 내용(Semantic Tokens)과 화자 특성(Global Tokens)으로 분리하며, 몇 초의 참조 음성만으로 Zero-shot Voice Cloning이 가능해요. Chain-of-Thought 방식으로 생성하며 성별, 음높이, 말하기 속도를 세밀하게 제어할 수 있어요. L20 GPU에서 거의 실시간(RTF 0.1362) 성능을 보이고, 100,000시간 VoxBox 데이터셋으로 학습됐어요.

<Callout type="info">
  **VoxBox 데이터셋**: 화자 속성이 상세히 주석된 100,000시간 분량으로 연구
  가치가 높아요.
</Callout>

</Step>

<Step>

### ComfyUI-WanVideoWrapper

<GithubInfo
  owner="kijai"
  repo="ComfyUI-WanVideoWrapper"
  token={process.env.NEXT_PUBLIC_GITHUB_TOKEN}
  className="not-prose"
/>

Wan 비디오 모델을 ComfyUI 노드 기반 워크플로우에 통합하는 래퍼예요. WanVideoBlockSwap으로 14B 모델을 12GB VRAM GPU에서도 실행 가능하게 만들고, TeaCache로 30-40% 속도 향상을 제공해요. MultiTalk, FantasyTalking, HuMo 등 오디오 기반 비디오 생성 시스템을 통합하며, 컨텍스트 윈도우로 81프레임(5초) 제한을 넘어 1025프레임(64초) 비디오도 생성할 수 있어요. RTX 5090으로 64초 비디오를 10분 만에, 5GB 미만 VRAM으로 처리했어요.

<Callout>
  **TeaCache 파라미터**: threshold 0.25-0.30 범위가 적당하고, coefficient 사용
  시 10배 높게 설정하세요.
</Callout>

</Step>

<Step>

### MaiBot

<GithubInfo
  owner="Mai-with-u"
  repo="MaiBot"
  token={process.env.NEXT_PUBLIC_GITHUB_TOKEN}
  className="not-prose"
/>

QQ 그룹 채팅에서 "살아있는 존재"처럼 행동하는 AI 에이전트예요. "helpful assistant"가 아닌 "생명 형태"를 목표로 하며, 실수도 하고 엉뚱한 답변도 해서 더 인간적으로 느껴져요. 행동 계획 시스템으로 언제 말할지 스스로 판단하고, 그룹마다 고유한 은어를 학습해 사용해요. 감정 상태 머신으로 대화 분위기에 따라 참여도가 달라지며, 플러그인 시스템으로 명령어 기반과 트리거 기반 확장이 가능해요. NapCat으로 QQ와 통합되고 WeChat, Telegram 등 멀티 플랫폼을 지원해요.

<Callout type="info">
  **프라이버시 철학**: "완전히 제어할 수 없는 개체여야 자율성이 느껴진다"는
  철학으로 실행 데이터를 폐쇄적으로 관리해요.
</Callout>

</Step>

</Steps>

---

**Sources:**

- [Beyond Anthropic: Using Claude Code with Any Model](https://lgallardo.com/2025/08/20/claude-code-router-openrouter-beyond-anthropic/)
- [Wan2.1: Best open-sourced AI Video generation model](https://medium.com/data-science-in-your-pocket/wan2-1-best-open-sourced-ai-video-generation-model-beats-openai-sora-6ea081cbb8f8)
- [Alibaba Releases Open-Source Video Generation Model Wan 2.1](https://analyticsindiamag.com/ai-news-updates/alibaba-releases-open-source-video-generation-model-wan-2-1-outperforms-openais-sora/)
- [Spark-TTS: An Efficient LLM-Based Text-to-Speech Model](https://arxiv.org/abs/2503.01710)
- [ComfyUI Wan2.1 Video Generation Workflow](https://docs.comfy.org/tutorials/video/wan/wan2_2)
